import io
import sys

import os
import cv2
import numpy as np

import psutil
import math
import random
import copy
import time
import pickle
from scipy.signal import correlate, convolve
class Conv2D:
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, lr=0.01):
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.lr = lr  #Th√™m d√≤ng n√†y ƒë·ªÉ l∆∞u learning rate

        # Xavier Initialization
        limit = np.sqrt(6 / (in_channels + out_channels))
        self.kernels = np.random.uniform(-limit, limit, 
                                         (out_channels, in_channels, kernel_size, kernel_size))

    def forward(self, input_tensor, training=False):
        print(f"[DEBUG] Conv2D Forward: Input shape {input_tensor.shape}")

        self.input = input_tensor  # L∆∞u ƒë·∫ßu v√†o ƒë·ªÉ backward
        batch_size, in_channels, height, width = input_tensor.shape

        output_height = (height - self.kernel_size + 2 * self.padding) // self.stride + 1
        output_width = (width - self.kernel_size + 2 * self.padding) // self.stride + 1

        output = np.zeros((batch_size, self.out_channels, output_height, output_width), dtype=np.float32)
        print(f"[DEBUG] Conv2D Output shape {output.shape}")

        if self.padding > 0:
            input_tensor = np.pad(input_tensor,
                                ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),
                                mode='constant')
            print(f"[DEBUG] Conv2D Input after padding {input_tensor.shape}")

        # T√≠ch ch·∫≠p t·ª´ng kernel
        for oc in range(self.out_channels):
            for ic in range(in_channels):
                kernel_reshaped = self.kernels[oc, ic][np.newaxis, :, :]
                #print(f"[DEBUG] Conv2D Kernel shape {kernel_reshaped.shape}")

                try:
                    output[:, oc] += correlate(input_tensor[:, ic], kernel_reshaped, mode='valid')
                except Exception as e:
                    print(f"[ERROR] Error while convolutional: {e}")
                    raise e

        self.output = output  # L∆∞u output ƒë·ªÉ d√πng trong backward
        print("[DEBUG] Conv2D Forward finished.")
        return output


    def backward(self, d_output):
        batch_size, in_channels, height, width = self.input.shape
        _, _, output_height, output_width = d_output.shape

        d_kernels = np.zeros_like(self.kernels)
        d_input = np.zeros_like(self.input)

        # Th√™m padding n·∫øu c√≥
        if self.padding > 0:
            padded_input = np.pad(self.input,
                                ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)),
                                mode='constant')
        else:
            padded_input = self.input

        # [DEBUG] Ki·ªÉm tra shape tr∆∞·ªõc khi t√≠nh to√°n gradient
        print(f"[DEBUG] d_output shape: {d_output.shape}")
        print(f"[DEBUG] padded_input shape: {padded_input.shape}")
        print(f"[DEBUG] d_input shape: {d_input.shape}")

        # T√≠nh gradient cho kernels (d_kernels)
        for oc in range(self.out_channels):
            for ic in range(in_channels):
                for b in range(batch_size):
                    d_kernels[oc, ic] += correlate(padded_input[b, ic], d_output[b, oc], mode='valid')

        # T√≠nh gradient cho input (d_input)
        for ic in range(in_channels):
            for oc in range(self.out_channels):
                flipped_kernel = np.flip(self.kernels[oc, ic], axis=(0, 1))
                for b in range(batch_size):
                    conv_result = convolve(d_output[b, oc], flipped_kernel, mode='full')

                    # ƒê·∫£m b·∫£o gradient c√≥ c√πng shape v·ªõi `d_input`
                    expected_shape = d_input.shape[2:]  # (height, width)
                    crop_h = (conv_result.shape[0] - expected_shape[0]) // 2
                    crop_w = (conv_result.shape[1] - expected_shape[1]) // 2

                    if crop_h > 0 or crop_w > 0:
                        conv_result = conv_result[crop_h:-crop_h, crop_w:-crop_w]

                    d_input[b, ic] += conv_result

        # C·∫≠p nh·∫≠t tr·ªçng s·ªë (Gradient Descent)
        self.kernels -= self.lr * d_kernels

        # Fix l·ªói: L∆∞u gradient ƒë·ªÉ c√°c layer sau s·ª≠ d·ª•ng
        self.dinputs = d_input  

        # [DEBUG] Ki·ªÉm tra shape sau khi backward
        print(f"[DEBUG] d_kernels shape: {d_kernels.shape}")
        print(f"[DEBUG] d_input shape: {d_input.shape}")

        return d_input

class Linear:
    def __init__(self, in_features, out_features):
        self.in_features = in_features
        self.out_features = out_features
        self.weights = np.random.uniform(-1, 1, (in_features, out_features))
        self.biases = np.random.uniform(-1, 1, (1, out_features))
        self.dweights = np.zeros_like(self.weights)
        self.dbiases = np.zeros_like(self.biases)

    def forward(self, input_tensor, training=False):  # Th√™m training=False
        """
        T√≠nh to√°n forward c·ªßa l·ªõp Linear (Fully Connected).
        :param input_tensor: NumPy array c√≥ shape (batch_size, in_features)
        :param training: C·ªù ki·ªÉm tra ƒëang training hay inference (kh√¥ng s·ª≠ d·ª•ng ·ªü ƒë√¢y)
        :return: NumPy array c√≥ shape (batch_size, out_features)
        """
        self.input_tensor = input_tensor  # L∆∞u input ƒë·ªÉ d√πng trong backward
        self.output = np.dot(input_tensor, self.weights) + self.biases
        return self.output

    def backward(self, d_loss):
        """
        Truy·ªÅn ng∆∞·ª£c gradient qua l·ªõp Linear.
        :param d_loss: Gradient c·ªßa loss ƒë·ªëi v·ªõi output c·ªßa Linear
        :return: Gradient c·ªßa loss ƒë·ªëi v·ªõi input c·ªßa Linear
        """
        print(f"[DEBUG] Linear backward input shape: {d_loss.shape}")  # üõ†Ô∏è Th√™m debug

        self.dweights = np.dot(self.input_tensor.T, d_loss)
        self.dbiases = np.sum(d_loss, axis=0, keepdims=True)
        
        self.dinputs = np.dot(d_loss, self.weights.T)  
        print(f"[DEBUG] Linear dinputs shape: {self.dinputs.shape}")

        if d_loss.shape[1] != self.weights.shape[1]:
            print(f"[WARNING] d_loss shape {d_loss.shape} not match, need reshape...")
            d_loss = d_loss.reshape(-1, self.weights.shape[1])  # Th·ª≠ reshape

        # Ki·ªÉm tra k√≠ch th∆∞·ªõc tr∆∞·ªõc khi nh√¢n ma tr·∫≠n
        assert self.weights.shape[1] == d_loss.shape[1], \
            f"[ERROR] Shape mismatch: {self.weights.shape} vs {d_loss.shape}"

        return np.dot(d_loss, self.weights.T)  # Tr·∫£ v·ªÅ gradient ƒë·ªÉ truy·ªÅn ng∆∞·ª£c


class Sigmoid:
    def forward(self, input_tensor, training=False):
        epsilon = 1e-9  # üî• Tr√°nh m·∫•t gradient
        self.output = 1 / (1 + np.exp(-np.clip(input_tensor, -500, 500)))
        self.output = np.clip(self.output, epsilon, 1 - epsilon)  # üî• Th√™m clipping
        print(f"[DEBUG] Sigmoid output (min: {self.output.min()}, max: {self.output.max()})")
        return self.output

    def backward(self, d_loss):
        """
        Truy·ªÅn ng∆∞·ª£c gradient qua Sigmoid.
        """
        self.dinputs = d_loss * (self.output * (1 - self.output))  # üî• Fix l·ªói `next`
        return self.dinputs

    def predictions(self, output_tensor):
        """
        Chuy·ªÉn ƒë·∫ßu ra th√†nh nh√£n nh·ªã ph√¢n (0 ho·∫∑c 1).
        """
        return output_tensor > 0.5  # N·∫øu > 0.5, tr·∫£ v·ªÅ 1; ng∆∞·ª£c l·∫°i, tr·∫£ v·ªÅ 0

class BatchNorm1D:
    def __init__(self, num_features, momentum=0.9, epsilon=1e-5):
        self.num_features = num_features
        self.momentum = momentum
        self.epsilon = epsilon

        self.gamma = np.ones((1, num_features))
        self.beta = np.zeros((1, num_features))

        self.running_mean = np.zeros((1, num_features))
        self.running_var = np.ones((1, num_features))

    def forward(self, input_tensor, training=True):
        self.input = input_tensor  # üî• L∆∞u input ƒë·ªÉ d√πng trong backward()
        
        if training:
            self.batch_mean = np.mean(input_tensor, axis=0, keepdims=True)
            self.batch_var = np.var(input_tensor, axis=0, keepdims=True)

            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.batch_mean
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.batch_var

            self.x_hat = (input_tensor - self.batch_mean) / np.sqrt(self.batch_var + self.epsilon)
        else:
            self.x_hat = (input_tensor - self.running_mean) / np.sqrt(self.running_var + self.epsilon)

        self.output = self.gamma * self.x_hat + self.beta
        return self.output

    def backward(self, d_loss):
        batch_size = d_loss.shape[0]

        d_gamma = np.sum(d_loss * self.x_hat, axis=0, keepdims=True)
        d_beta = np.sum(d_loss, axis=0, keepdims=True)

        d_x_hat = d_loss * self.gamma
        d_var = np.sum(d_x_hat * (self.input - self.batch_mean) * -0.5 * np.power(self.batch_var + self.epsilon, -1.5), axis=0, keepdims=True)
        d_mean = np.sum(d_x_hat * -1 / np.sqrt(self.batch_var + self.epsilon), axis=0, keepdims=True) + d_var * np.mean(-2 * (self.input - self.batch_mean), axis=0, keepdims=True)

        self.dinputs = d_x_hat / np.sqrt(self.batch_var + self.epsilon) + d_var * 2 * (self.input - self.batch_mean) / batch_size + d_mean / batch_size
        return self.dinputs


class ReLU:
    def forward(self, input_tensor, training=False):
        """
        √Åp d·ª•ng h√†m ReLU tr√™n tensor ƒë·∫ßu v√†o.
        :param input_tensor: NumPy array c√≥ shape b·∫•t k·ª≥
        :param training: C·ªù ki·ªÉm tra ƒëang training hay inference (kh√¥ng s·ª≠ d·ª•ng ·ªü ƒë√¢y)
        :return: NumPy array sau khi √°p d·ª•ng ReLU
        """
        self.mask = input_tensor > 0  # üî• L∆∞u mask nh·ªã ph√¢n thay v√¨ input ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
        self.output = np.maximum(0, input_tensor)
        return self.output

    def backward(self, d_loss):
        """
        Truy·ªÅn ng∆∞·ª£c gradient qua ReLU.
        :param d_loss: Gradient c·ªßa loss ƒë·ªëi v·ªõi output c·ªßa ReLU
        :return: Gradient c·ªßa loss ƒë·ªëi v·ªõi input c·ªßa ReLU
        """
        print(f"[DEBUG] ReLU backward input shape: {d_loss.shape}")  # üõ†Ô∏è Debug

        self.dinputs = d_loss * self.mask  # üî• S·ª≠ d·ª•ng mask ƒë·ªÉ t√≠nh gradient
        print(f"[DEBUG] ReLU dinputs shape: {self.dinputs.shape}")  # üõ†Ô∏è Debug

        return self.dinputs

class MaxPool2D:
    def __init__(self, kernel_size=2, stride=2):
        self.kernel_size = kernel_size
        self.stride = stride
    def forward(self, input_tensor, training=False):
        batch_size, channels, height, width = input_tensor.shape
        pool_height, pool_width = self.kernel_size, self.kernel_size
        stride = self.stride

        output_height = (height - pool_height) // stride + 1
        output_width = (width - pool_width) // stride + 1

        self.input_shape = input_tensor.shape
        self.output = np.zeros((batch_size, channels, output_height, output_width), dtype=np.float32)

        # üî• D√πng NumPy thay v√¨ dict ƒë·ªÉ l∆∞u ch·ªâ m·ª•c max
        self.max_indices = np.zeros((batch_size, channels, output_height, output_width, 2), dtype=np.int32)

        for b in range(batch_size):
            for c in range(channels):
                for i in range(output_height):
                    for j in range(output_width):
                        start_i = i * stride
                        start_j = j * stride
                        end_i = start_i + pool_height
                        end_j = start_j + pool_width

                        region = input_tensor[b, c, start_i:end_i, start_j:end_j]
                        max_pos = np.unravel_index(np.argmax(region), region.shape)

                        self.max_indices[b, c, i, j] = [start_i + max_pos[0], start_j + max_pos[1]]
                        self.output[b, c, i, j] = region[max_pos]

        return self.output
    
    def backward(self, d_loss):
        batch_size, channels, height, width = self.input_shape
        self.dinputs = np.zeros((batch_size, channels, height, width), dtype=np.float32)  # üî• L∆∞u `self.dinputs`

        output_height, output_width = d_loss.shape[2], d_loss.shape[3]

        for b in range(batch_size):
            for c in range(channels):
                for i in range(output_height):
                    for j in range(output_width):
                        orig_i, orig_j = self.max_indices[b, c, i, j]  # üî• L·∫•y ch·ªâ s·ªë max t·ª´ NumPy array
                        self.dinputs[b, c, orig_i, orig_j] = d_loss[b, c, i, j]

        return self.dinputs
class AdaptiveAvgPool2D:
    def __init__(self, output_size):
        """
        L·ªõp Adaptive Average Pooling.
        :param output_size: K√≠ch th∆∞·ªõc ƒë·∫ßu ra mong mu·ªën (H_out, W_out).
        """
        self.output_size = output_size  # (H_out, W_out)

    def forward(self, input_tensor, training=False):  # Th√™m training=False ƒë·ªÉ tr√°nh l·ªói
        """
        Th·ª±c hi·ªán Adaptive Average Pooling.
        :param input_tensor: NumPy array c√≥ shape (batch_size, channels, height, width)
        :param training: C·ªù ki·ªÉm tra ƒëang training hay inference (kh√¥ng s·ª≠ d·ª•ng ·ªü ƒë√¢y)
        :return: NumPy array v·ªõi shape (batch_size, channels, H_out, W_out)
        """
        self.input_shape = input_tensor.shape  # L∆∞u l·∫°i shape ƒë·ªÉ d√πng trong backward
        batch_size, channels, height, width = input_tensor.shape
        H_out, W_out = self.output_size

        self.kernel_size_h = height // H_out
        self.kernel_size_w = width // W_out
        self.stride_h = self.kernel_size_h
        self.stride_w = self.kernel_size_w

        output_tensor = np.zeros((batch_size, channels, H_out, W_out))

        for i in range(H_out):
            for j in range(W_out):
                window = input_tensor[:, :, i * self.stride_h:i * self.stride_h + self.kernel_size_h,
                                      j * self.stride_w:j * self.stride_w + self.kernel_size_w]
                output_tensor[:, :, i, j] = np.mean(window, axis=(2, 3))  # T√≠nh trung b√¨nh nhanh h∆°n

        self.output = output_tensor  #**Fix l·ªói: L∆∞u output ƒë·ªÉ l·ªõp sau c√≥ th·ªÉ s·ª≠ d·ª•ng**
        return output_tensor

    def backward(self, d_loss):
        batch_size, channels, height, width = self.input_shape
        self.dinputs = np.zeros((batch_size, channels, height, width))  # üî• Fix: L∆∞u `self.dinputs`

        for i in range(self.output_size[0]):
            for j in range(self.output_size[1]):
                self.dinputs[:, :, i * self.stride_h:i * self.stride_h + self.kernel_size_h,
                            j * self.stride_w:j * self.stride_w + self.kernel_size_w] += (
                    d_loss[:, :, i, j][:, :, None, None] / (self.kernel_size_h * self.kernel_size_w)
                )

        return self.dinputs  # üî• Fix: Tr·∫£ v·ªÅ `self.dinputs`


class Dropout:
    def __init__(self, p=0.5):
        """
        L·ªõp Dropout ƒë·ªÉ regularization.
        :param p: X√°c su·∫•t b·ªè qua m·ªôt neuron (0 ‚â§ p < 1), m·∫∑c ƒë·ªãnh l√† 0.5.
        """
        assert 0 <= p < 1, "p ph·∫£i n·∫±m trong kho·∫£ng [0, 1)"
        self.p = p
        self.training = True  # M·∫∑c ƒë·ªãnh l√† training mode
        self.mask = None  # Mask s·∫Ω ƒë∆∞·ª£c t·∫°o trong forward

    def forward(self, input_tensor, training=True):  # Th√™m training=True
        """
        √Åp d·ª•ng Dropout l√™n input.
        :param input_tensor: NumPy array (batch_size, features)
        :param training: C·ªù ki·ªÉm tra ƒëang training hay inference
        :return: NumPy array sau khi √°p d·ª•ng Dropout
        """
        self.training = training  # C·∫≠p nh·∫≠t tr·∫°ng th√°i training

        if not self.training:
            return input_tensor  # Kh√¥ng dropout khi inference

        # T·∫°o mask (1 gi·ªØ l·∫°i, 0 b·ªè ƒëi), chia (1 - p) ƒë·ªÉ gi·ªØ gi√° tr·ªã k√¨ v·ªçng
        self.mask = (np.random.rand(*input_tensor.shape) > self.p) / (1 - self.p)
        self.output = input_tensor * self.mask  # **Fix l·ªói: L∆∞u output ƒë·ªÉ l·ªõp sau c√≥ th·ªÉ s·ª≠ d·ª•ng**
        return self.output
    
    def backward(self, d_loss):
        print(f"[DEBUG] Dropout backward input shape: {d_loss.shape}")  # üõ†Ô∏è Debug

        self.dinputs = d_loss * self.mask  # üî• Fix l·ªói: L∆∞u gradient l·∫°i
        print(f"[DEBUG] Dropout dinputs shape: {self.dinputs.shape}")  # üõ†Ô∏è Debug

        return self.dinputs



    def eval(self):
        """Chuy·ªÉn sang ch·∫ø ƒë·ªô inference (kh√¥ng Dropout)."""
        self.training = False

    def train(self):
        """Chuy·ªÉn sang ch·∫ø ƒë·ªô training (c√≥ Dropout)."""
        self.training = True


class DataLoader:
    def __init__(self, images, labels, batch_size=32, shuffle=True, drop_last=False):
        """
        DataLoader ƒë·ªÉ t·∫£i d·ªØ li·ªáu theo batch.
        
        :param images: NumPy array ho·∫∑c list ch·ª©a ·∫£nh.
        :param labels: NumPy array ho·∫∑c list ch·ª©a nh√£n.
        :param batch_size: K√≠ch th∆∞·ªõc batch.
        :param shuffle: C√≥ tr·ªôn d·ªØ li·ªáu hay kh√¥ng.
        :param drop_last: N·∫øu True, b·ªè batch cu·ªëi n·∫øu kh√¥ng ƒë·ªß batch_size.
        """
        self.images = np.array(images, dtype=np.float32)  # Chuy·ªÉn v·ªÅ float32
        self.labels = np.array(labels)
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.drop_last = drop_last
        self.indexes = np.arange(len(self.images))

    def __len__(self):
        """Tr·∫£ v·ªÅ s·ªë batch c√≥ th·ªÉ l·∫•y ƒë∆∞·ª£c."""
        total_batches = len(self.images) // self.batch_size
        if not self.drop_last and len(self.images) % self.batch_size != 0:
            total_batches += 1  # Th√™m batch l·∫ª n·∫øu kh√¥ng drop_last
        return total_batches

    def __iter__(self):
        """Kh·ªüi t·∫°o iterator."""
        if self.shuffle:
            np.random.shuffle(self.indexes)
        self.current_index = 0
        return self

    def __next__(self):
        """L·∫•y batch ti·∫øp theo."""
        if self.current_index >= len(self.images):
            raise StopIteration

        batch_indexes = self.indexes[self.current_index:self.current_index + self.batch_size]
        batch_images = self.images[batch_indexes]
        batch_labels = self.labels[batch_indexes]

        self.current_index += self.batch_size

        # N·∫øu drop_last v√† batch kh√¥ng ƒë·ªß size, b·ªè qua batch n√†y
        if self.drop_last and batch_images.shape[0] < self.batch_size:
            raise StopIteration

        return batch_images, batch_labels

class BinaryCrossEntropy:
    def __init__(self, reduction='mean'):
        self.reduction = reduction
        self.trainable_layers = []  # L∆∞u c√°c layer c√≥ tr·ªçng s·ªë
        self.accumulated_sum = 0
        self.accumulated_count = 0

    def remember_trainable_layers(self, trainable_layers):
        """L∆∞u danh s√°ch c√°c layer c√≥ tr·ªçng s·ªë ƒë·ªÉ t√≠nh regularization."""
        self.trainable_layers = trainable_layers

    def new_pass(self):
        """Reset loss t√≠ch l≈©y tr∆∞·ªõc m·ªói epoch."""
        self.accumulated_sum = 0
        self.accumulated_count = 0

    def forward(self, y_pred, y_true):
        print(f"[DEBUG] y_pred shape: {y_pred.shape}")  # üîç Ki·ªÉm tra shape
        print(f"[DEBUG] y_true shape before reshape: {y_true.shape}")  # üîç Ki·ªÉm tra tr∆∞·ªõc reshape
        print(f"[DEBUG] y_pred (min: {y_pred.min()}, max: {y_pred.max()})")
        print(f"[DEBUG] y_true unique values: {np.unique(y_true)}")  # Ki·ªÉm tra nh√£n
        # ƒê·∫£m b·∫£o `y_true` c√≥ shape (batch_size, 1)
        if y_true.ndim == 1:
            y_true = y_true.reshape(-1, 1)

        print(f"[DEBUG] y_true shape after reshape: {y_true.shape}")  # üîç Ki·ªÉm tra sau reshape
        assert y_true.shape == y_pred.shape, "[ERROR] y_true shape kh√¥ng kh·ªõp v·ªõi y_pred!"

        epsilon = 1e-9
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

        return np.mean(loss)

    def backward(self, y_pred, y_true):
        epsilon = 1e-9  # Tr√°nh chia cho 0
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

        self.dinputs = -(y_true / y_pred - (1 - y_true) / (1 - y_pred)) / len(y_true)  # üî• Fix: L∆∞u `self.dinputs`


    def calculate(self, output, y, include_regularization=False):
        """
        T√≠nh loss v√† h·ªó tr·ª£ regularization n·∫øu c√≥.
        """
        data_loss = self.forward(output, y)
        reg_loss = 0
        if include_regularization:
            for layer in self.trainable_layers:
                if hasattr(layer, 'weights'):
                    reg_loss += np.sum(layer.weights ** 2) * 0.0001  # L2 Regularization

        return data_loss + reg_loss

    def calculate_accumulated(self):
        """Tr·∫£ v·ªÅ loss trung b√¨nh sau khi accumulate qua nhi·ªÅu batch."""
        if self.accumulated_count == 0:
            return 0
        return self.accumulated_sum / self.accumulated_count

    
class SGD:
    def __init__(self, learning_rate=0.01, momentum=0.0, weight_decay=0.0, nesterov=False):
        """
        T·ªëi ∆∞u h√≥a SGD v·ªõi Momentum, Weight Decay v√† Nesterov Momentum.
        
        :param learning_rate: T·ªëc ƒë·ªô h·ªçc.
        :param momentum: H·ªá s·ªë momentum (0 = kh√¥ng d√πng momentum).
        :param weight_decay: H·ªá s·ªë weight decay (L2 Regularization).
        :param nesterov: S·ª≠ d·ª•ng Nesterov momentum n·∫øu True.
        """
        self.learning_rate = learning_rate
        self.momentum = momentum
        self.weight_decay = weight_decay
        self.nesterov = nesterov
        self.velocity = {}  # L∆∞u v·∫øt momentum

    def pre_update_params(self):
        """G·ªçi tr∆∞·ªõc khi c·∫≠p nh·∫≠t tham s·ªë (c√≥ th·ªÉ th√™m decay)."""
        pass

    def update_params(self, layer):
        """
        C·∫≠p nh·∫≠t tr·ªçng s·ªë c·ªßa layer theo SGD.
        
        :param layer: Layer c·∫ßn c·∫≠p nh·∫≠t (ph·∫£i c√≥ `weights` v√† `biases`).
        """
        if not hasattr(layer, 'weights') or not hasattr(layer, 'dweights'):
            return  # B·ªè qua n·∫øu layer kh√¥ng c√≥ tr·ªçng s·ªë

        # Weight decay (L2 Regularization)
        if self.weight_decay:
            layer.dweights += self.weight_decay * layer.weights

        # Kh·ªüi t·∫°o velocity n·∫øu ch∆∞a c√≥
        if layer not in self.velocity:
            self.velocity[layer] = {
                "w": np.zeros_like(layer.weights),
                "b": np.zeros_like(layer.biases)
            }

        # L·∫•y velocity hi·ªán t·∫°i
        v_w, v_b = self.velocity[layer]["w"], self.velocity[layer]["b"]

        # Momentum update
        if self.momentum > 0:
            v_w = self.momentum * v_w - self.learning_rate * layer.dweights
            v_b = self.momentum * v_b - self.learning_rate * layer.dbiases

            if self.nesterov:
                layer.weights += self.momentum * v_w - self.learning_rate * layer.dweights
                layer.biases += self.momentum * v_b - self.learning_rate * layer.dbiases
            else:
                layer.weights += v_w
                layer.biases += v_b

            # C·∫≠p nh·∫≠t velocity
            self.velocity[layer]["w"], self.velocity[layer]["b"] = v_w, v_b
        else:
            # SGD th∆∞·ªùng
            layer.weights -= self.learning_rate * layer.dweights
            layer.biases -= self.learning_rate * layer.dbiases

    def post_update_params(self):
        """G·ªçi sau khi c·∫≠p nh·∫≠t tham s·ªë (c√≥ th·ªÉ th√™m adaptive LR)."""
        pass

class Accuracy:
    def __init__(self):
        self.accumulated_sum = 0
        self.accumulated_count = 0

    def new_pass(self):
        """Reset accuracy t√≠ch l≈©y tr∆∞·ªõc m·ªói epoch."""
        self.accumulated_sum = 0
        self.accumulated_count = 0

    def init(self, y_true):
        """Kh·ªüi t·∫°o l·∫°i c√°c gi√° tr·ªã t√≠ch l≈©y."""
        self.new_pass()

    def calculate(self, predictions, y_true):
        """T√≠nh accuracy d·ª±a tr√™n d·ª± ƒëo√°n."""
        predictions = predictions > 0.5  # Chuy·ªÉn th√†nh d·∫°ng nh·ªã ph√¢n
        accuracy = np.mean(predictions == y_true)

        # C·ªông d·ªìn accuracy ƒë·ªÉ t√≠nh trung b√¨nh sau n√†y
        self.accumulated_sum += accuracy
        self.accumulated_count += 1

        return accuracy

    def calculate_accumulated(self):
        """Tr·∫£ v·ªÅ gi√° tr·ªã accuracy trung b√¨nh qua nhi·ªÅu batch."""
        if self.accumulated_count == 0:
            return 0
        return self.accumulated_sum / self.accumulated_count


class Layer_Input:
    def __init__(self, input_shape=None):
        """
        L·ªõp ƒë·∫ßu v√†o ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu ƒë·∫ßu v√†o v√†o m·∫°ng.
        
        :param input_shape: Tuple m√¥ t·∫£ k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o (t√πy ch·ªçn).
        """
        self.input_shape = input_shape

    def forward(self, inputs):
        """
        Forward pass - ch·ªâ c·∫ßn l∆∞u l·∫°i ƒë·∫ßu v√†o.
        
        :param inputs: D·ªØ li·ªáu ƒë·∫ßu v√†o c·ªßa m·∫°ng.
        """
        self.output = inputs

    def backward(self, d_output):
        """
        Backward pass - kh√¥ng l√†m g√¨ v√¨ ƒë√¢y l√† l·ªõp ƒë·∫ßu v√†o.
        
        :param d_output: Gradient ƒë·∫ßu ra t·ª´ l·ªõp ti·∫øp theo (kh√¥ng s·ª≠ d·ª•ng ·ªü ƒë√¢y).
        """
        self.dinputs = d_output  # Ch·ªâ c·∫ßn truy·ªÅn gradient ti·∫øp t·ª•c v·ªÅ ph√≠a tr∆∞·ªõc

class Flatten:
    def forward(self, input_tensor, training=False):
        """
        Chuy·ªÉn ƒë·ªïi tensor ƒë·∫ßu v√†o th√†nh d·∫°ng vector ph·∫≥ng.
        :param input_tensor: NumPy array c√≥ shape (batch_size, C, H, W)
        :return: NumPy array c√≥ shape (batch_size, C * H * W)
        """
        self.input_shape = input_tensor.shape  # üî• L∆∞u shape ƒë·ªÉ d√πng trong backward
        self.output = input_tensor.reshape(input_tensor.shape[0], -1)  # Chuy·ªÉn th√†nh vector 2D
        return self.output

    def backward(self, d_loss):
        """
        Truy·ªÅn ng∆∞·ª£c gradient qua Flatten.
        :param d_loss: Gradient t·ª´ l·ªõp ti·∫øp theo
        :return: Gradient reshaped v·ªÅ l·∫°i k√≠ch th∆∞·ªõc ban ƒë·∫ßu
        """
        print(f"[DEBUG] Flatten backward input shape: {d_loss.shape}")  # üõ†Ô∏è Debug

        self.dinputs = d_loss.reshape(self.input_shape)  # üî• Kh√¥i ph·ª•c shape ban ƒë·∫ßu
        print(f"[DEBUG] Flatten dinputs shape: {self.dinputs.shape}")  # üõ†Ô∏è Debug

        return self.dinputs



class Model:
    def __init__(self):
        self.layers = []
        self.trainable_layers = []
        self.input_layer = None
        self.output_layer_activation = None
        self.loss = None
        self.optimizer = None
        self.accuracy = None

    def add(self, layer):
        """Th√™m m·ªôt l·ªõp v√†o m√¥ h√¨nh."""
        self.layers.append(layer)
    
    def set(self, *, loss, optimizer, accuracy):
        """Thi·∫øt l·∫≠p loss, optimizer v√† accuracy metric."""
        self.loss = loss
        self.optimizer = optimizer
        self.accuracy = accuracy

    def finalize(self):
        if not self.layers:
            raise ValueError("Model has no layers. Please add layers before finalizing.")
        if self.loss is None:
            raise ValueError("Loss function is not set. Use model.set(loss=...) before finalizing.")

        self.input_layer = Layer_Input()

        for i in range(len(self.layers)):
            if i == 0:
                self.layers[i].prev = self.input_layer
            else:
                self.layers[i].prev = self.layers[i - 1]
                self.layers[i - 1].next = self.layers[i]

        self.output_layer_activation = self.layers[-1]
        self.trainable_layers = [layer for layer in self.layers if hasattr(layer, 'weights')]

        self.loss.remember_trainable_layers(self.trainable_layers)

    def forward(self, X, training=False):
        """Truy·ªÅn d·ªØ li·ªáu qua m·∫°ng (forward pass)."""
        print("[DEBUG] Forward start...")

        self.input_layer.forward(X)
        print(f"[DEBUG] Input shape: {X.shape}")

        for i, layer in enumerate(self.layers):
            print(f"[DEBUG] Forward over layer {i}: {layer.__class__.__name__}")

            try:
                layer.forward(layer.prev.output, training)
            except Exception as e:
                print(f"[ERROR] Issue at layer {i}: {layer.__class__.__name__} - {e}")
                raise e  # D·ª´ng ch∆∞∆°ng tr√¨nh v√† hi·ªÉn th·ªã l·ªói

        print("[DEBUG] Forward finished.")
        return self.output_layer_activation.output


    def backward(self, output, y_true):
        """Truy·ªÅn ng∆∞·ª£c l·ªói qua m·∫°ng (backward pass)."""

        print("[DEBUG] Backward start...")

        # Backpropagate qua Loss
        self.loss.backward(output, y_true)

        # Backpropagate qua l·ªõp cu·ªëi c√πng tr∆∞·ªõc (Sigmoid)
        self.output_layer_activation.backward(self.loss.dinputs)

        # Truy·ªÅn ng∆∞·ª£c qua c√°c l·ªõp c√≤n l·∫°i
        for layer in reversed(self.layers[:-1]):  # Lo·∫°i b·ªè l·ªõp cu·ªëi Sigmoid
            print(f"[DEBUG] Backward over layer {layer.__class__.__name__}")

            try:
                layer.backward(layer.next.dinputs)
            except Exception as e:
                print(f"[ERROR] issue at layer {layer.__class__.__name__}: {e}")
                raise e  # In l·ªói chi ti·∫øt


    def train(self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data=None):
        self.accuracy.init(y)

        train_steps = len(X) // batch_size if batch_size else 1
        if batch_size and train_steps * batch_size < len(X):
            train_steps += 1

        print(f"Train steps per epoch: {train_steps}, Batch size: {batch_size}")

        for epoch in range(1, epochs + 1):
            print(f"\nEpoch {epoch}/{epochs}")
            self.loss.new_pass()
            self.accuracy.new_pass()

            for step in range(train_steps):
                start_time = time.time()

                print(f"Preparing batch {step+1}/{train_steps}...")
                batch_X = X[step * batch_size:(step + 1) * batch_size] if batch_size else X
                batch_y = y[step * batch_size:(step + 1) * batch_size] if batch_size else y

                print(f"Batch shape: {batch_X.shape}, Labels shape: {batch_y.shape}")
                
                if batch_y.ndim == 1:
                    batch_y = batch_y.reshape(-1, 1)
                
                print("Running forward pass...")
                output = self.forward(batch_X, training=True)
                print("Forward pass done.")

                print("Calculating loss and accuracy...")
                loss = self.loss.calculate(output, batch_y, include_regularization=True)
                accuracy = self.accuracy.calculate(self.output_layer_activation.predictions(output), batch_y)
                print(f"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}")

                print("Running backward pass...")
                self.backward(output, batch_y)
                print("Backward pass done.")

                if self.optimizer:
                    print("Updating parameters...")
                    self.optimizer.pre_update_params()
                    for layer in self.trainable_layers:
                        self.optimizer.update_params(layer)
                    self.optimizer.post_update_params()
                    print("Parameters updated.")

                end_time = time.time()
                print(f"Step {step+1}/{train_steps} - Time: {end_time - start_time:.4f} sec")
                
                if step % print_every == 0 or step == train_steps - 1:
                    print(f"Step {step}/{train_steps}, acc: {accuracy:.3f}, loss: {loss:.3f}")

            print(f"Epoch {epoch} completed - acc: {self.accuracy.calculate_accumulated():.3f}, loss: {self.loss.calculate_accumulated():.3f}")

            if validation_data:
                print("Running validation...")
                self.evaluate(*validation_data, batch_size=batch_size)
                print("Validation completed.")


    def evaluate(self, X_val, y_val, *, batch_size=None):
        validation_steps = len(X_val) // batch_size if batch_size else 1
        if batch_size and validation_steps * batch_size < len(X_val):
            validation_steps += 1

        self.loss.new_pass()
        self.accuracy.new_pass()

        for step in range(validation_steps):
            batch_X = X_val[step * batch_size:(step + 1) * batch_size] if batch_size else X_val
            batch_y = y_val[step * batch_size:(step + 1) * batch_size] if batch_size else y_val

            output = self.forward(batch_X, training=False)
            self.loss.calculate(output, batch_y)
            self.accuracy.calculate(self.output_layer_activation.predictions(output), batch_y)

        print(f'Validation - Accuracy: {self.accuracy.calculate_accumulated():.3f}, Loss: {self.loss.calculate_accumulated():.3f}')

    def predict(self, X, *, batch_size=None):
        prediction_steps = len(X) // batch_size if batch_size else 1
        if batch_size and prediction_steps * batch_size < len(X):
            prediction_steps += 1

        output = []
        for step in range(prediction_steps):
            batch_X = X[step * batch_size:(step + 1) * batch_size] if batch_size else X
            batch_output = self.forward(batch_X, training=False)
            output.append(batch_output)

        return np.vstack(output)

    def save(self, path):
        """L∆∞u m√¥ h√¨nh v√†o file."""
        model = copy.deepcopy(self)
        model.loss.new_pass()
        model.accuracy.new_pass()

        for layer in model.layers:
            for attr in ['inputs', 'output', 'dinputs', 'dweights', 'dbiases']:
                layer.__dict__.pop(attr, None)

        with open(path, 'wb') as f:
            pickle.dump(model, f)

    @staticmethod
    def load(path):
        """T·∫£i m√¥ h√¨nh t·ª´ file."""
        with open(path, 'rb') as f:
            return pickle.load(f)


class VGG16(Model):
    def __init__(self, num_classes=1, lr=0.01):
        super().__init__()

        self.add(Conv2D(3, 64, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(Conv2D(64, 64, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(MaxPool2D(kernel_size=2, stride=2))

        self.add(Conv2D(64, 128, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(Conv2D(128, 128, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(MaxPool2D(kernel_size=2, stride=2))

        self.add(Conv2D(128, 256, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(Conv2D(256, 256, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(Conv2D(256, 256, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(MaxPool2D(kernel_size=2, stride=2))

        self.add(Conv2D(256, 512, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(Conv2D(512, 512, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(Conv2D(512, 512, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(MaxPool2D(kernel_size=2, stride=2))

        self.add(Conv2D(512, 512, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(Conv2D(512, 512, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(Conv2D(512, 512, kernel_size=3, stride=1, padding=1, lr=lr))
        self.add(ReLU())
        self.add(MaxPool2D(kernel_size=2, stride=2))

        self.add(AdaptiveAvgPool2D((7, 7)))
        self.add(Flatten())

        self.add(Linear(512 * 7 * 7, 4096))
        self.add(BatchNorm1D(4096))  # Th√™m BatchNorm
        self.add(ReLU())
        self.add(Dropout(0.5))

        self.add(Linear(4096, 4096))
        self.add(BatchNorm1D(4096))  # Th√™m BatchNorm
        self.add(ReLU())
        self.add(Dropout(0.5))

        self.add(Linear(4096, num_classes))
        self.add(Sigmoid())

    def forward(self, x, training=False):
        return super().forward(x, training=training)

    def backward(self, output, y_true):
        return super().backward(output, y_true)


##############################################################################################


# H√†m load ·∫£nh t·ª´ th∆∞ m·ª•c
def load_images_from_folder(folder, label, image_size=(224, 224)):
    images = []
    labels = []
    
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)  # ƒê·ªçc ·∫£nh
        if img is not None:
            img = cv2.resize(img, image_size)  # Resize v·ªÅ 224x224
            img = img / 255.0  # Chu·∫©n h√≥a v·ªÅ [0,1]
            img = np.transpose(img, (2, 0, 1))  # Chuy·ªÉn t·ª´ (224, 224, 3) -> (3, 224, 224)
            images.append(img)
            labels.append(label)  # Gi·ªØ nguy√™n nh√£n d·∫°ng int
    
    return images, labels

# M·ªü file ƒë·ªÉ ghi
log_file = open("output13.log", "w")

# Ghi c·∫£ stdout v√† stderr v√†o file
sys.stdout = log_file
sys.stderr = log_file

print("Run program...")  # C√¢u n√†y s·∫Ω ghi v√†o file

# Check RAM status
available_memory = psutil.virtual_memory().available / (1024 * 1024)
print(f"[INFO] Available RAM: {available_memory:.2f} MB")

# Kh·ªüi t·∫°o Model
model = VGG16(num_classes=1, lr=0.01)

# Set loss, optimizer, accuracy
loss_function = BinaryCrossEntropy()
optimizer = SGD(learning_rate=0.001)
accuracy = Accuracy()

# C·∫•u h√¨nh model
model.set(loss=loss_function, optimizer=optimizer, accuracy=accuracy)
model.finalize()

# Load d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c
meningioma_images, meningioma_labels = load_images_from_folder('C:/Personal/final_graduate/data/meningioma', label=1)
non_meningioma_images, non_meningioma_labels = load_images_from_folder('C:/Personal/final_graduate/data/notumor', label=0)


# G·ªôp t·∫•t c·∫£ d·ªØ li·ªáu l·∫°i v√† shuffle
images = np.array(meningioma_images + non_meningioma_images)
labels = np.array(meningioma_labels + non_meningioma_labels)

print(f"Total images: {len(images)}, Total labels: {len(labels)}")
print(f"Image shape: {images[0].shape}")

# Shuffle d·ªØ li·ªáu
indices = np.arange(len(images))
np.random.shuffle(indices)
images, labels = images[indices], labels[indices]

# Train m√¥ h√¨nh (S·ª≠ d·ª•ng h√†m train c√≥ s·∫µn trong Model)
model.train(images, labels, epochs=10, batch_size=8, print_every=1)

# L∆∞u m√¥ h√¨nh sau khi train
model.save("brain_tumor_vgg16.pkl")


log_file.close()  # ƒê√≥ng file sau khi ho√†n t·∫•t
